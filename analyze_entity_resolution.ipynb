{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Analyze\n",
    "In this demonstration, we analyze entity resolution as a downstreaming application. We show that the tables integrated using ALITE prepares a better ground for entity resolution than the tables integrated using outer join operator. The results are reported in the form of precision, recall and F-score of entity resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_entitymatching as em\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('python version: ' + sys.version )\n",
    "print('pandas version: ' + pd.__version__ )\n",
    "print('magellan version: ' + em.__version__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'alite_fd_stadiums.csv'\n",
    "#file = 'stadiums_minimum_oj.csv'\n",
    "#path_A = em.get_install_path() + os.sep + 'em_inputs' + os.sep + file\n",
    "#path_B = em.get_install_path() + os.sep + 'em_inputs' + os.sep + file\n",
    "path_A = 'em_inputs' + os.sep + file\n",
    "path_B = 'em_inputs' + os.sep + file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = em.read_csv_metadata(path_A, key = \"index\")\n",
    "B = em.read_csv_metadata(path_B, key = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of tuples in A: ' + str(len(A)))\n",
    "print('Number of tuples in B: ' + str(len(B)))\n",
    "print('Number of tuples in A X B (i.e the cartesian product): ' + str(len(A)*len(B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize overlap blocker\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "ob = em.OverlapBlocker()\n",
    "# Block over title attribute\n",
    "#attrs = ['name','capacity','location','opened','team']\n",
    "#attrs = ['player', 'position', 'team', 'facility', 'location', 'capacity', 'opened']\n",
    "attrs = ['player', 'team', 'facility']\n",
    "\n",
    "C = ob.block_tables(A, B, 'facility', 'facility', \n",
    "                    l_output_attrs=attrs,\n",
    "                    r_output_attrs=attrs, \n",
    "                    show_progress=False, overlap_size=2)\n",
    "len(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = em.sample_table(C, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G = em.label_table(S, label_column_name='gold_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_f = em.get_features_for_matching(A, B, validate_inferred_attr_types = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brm = em.BooleanRuleMatcher()\n",
    "rule_1 = ['player_player_jac_dlm_dc0_dlm_dc0(ltuple, rtuple) > 0.3', 'player_player_cos_dlm_dc0_dlm_dc0(ltuple, rtuple) > 0.3', 'player_player_lev_sim(ltuple, rtuple) > 0.3']\n",
    "#rule_2 = ['facility_facility_jac_dlm_dc0_dlm_dc0(ltuple, rtuple) > 0.8', 'facility_facility_cos_dlm_dc0_dlm_dc0(ltuple, rtuple)> 0.8', 'facility_facility_lev_sim(ltuple, rtuple) > 0.8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rule_name = brm.add_rule(rule_1, match_f)\n",
    "#rule_name = brm.add_rule(rule_2, match_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = brm.predict(table=C, target_attr='predicted_labels', inplace=True)\n",
    "C['predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CC  = C[C['ltable_index'] != C['rtable_index']]\n",
    "CCC = CC[['ltable_index', 'rtable_index', 'predictions']]\n",
    "final_cols = list(CCC.columns)\n",
    "final_rows = []\n",
    "indexed_rows = set()\n",
    "for index, rows in CCC.iterrows():\n",
    "    ltable = rows['ltable_index']\n",
    "    rtable = rows['rtable_index']\n",
    "    predictions = rows['predictions']\n",
    "    if (ltable, rtable) not in indexed_rows and (rtable, ltable) not in indexed_rows:\n",
    "        final_rows.append((ltable, rtable, predictions))\n",
    "        indexed_rows.add((ltable, rtable))\n",
    "final_dataframe = pd.DataFrame(final_rows, columns= final_cols )     \n",
    "#CCC = pd.DataFrame(np.sort(CCC.values, axis=1), columns=CCC.columns).drop_duplicates(ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe.to_csv(\"em_outputs/em_result_\"+file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_tuples = set()\n",
    "\n",
    "for index, row in final_dataframe.iterrows():\n",
    "    if row['predictions'] == 1:\n",
    "        remove_tuples.add(min(row['ltable_index'], row['rtable_index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltable_cleaned = A[~A.index.isin(remove_tuples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltable_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare groundtruth for comparison. Since the partitioned tables may not have complete information, we only include those columns for the evaluation whose information are available (the columns participating on joins.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gt_table \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../data/em_gold/em_stadium_gold_complete_dirty.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m dirty_groundtruth \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m gt_table\u001b[39m.\u001b[39miterrows():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "gt_table = pd.read_csv(r\"../data/em_gold/em_stadium_gold_complete_dirty.csv\")\n",
    "dirty_groundtruth = set()\n",
    "for index, row in gt_table.iterrows():\n",
    "    player = row['Player'].lower()\n",
    "    team = row['Team'].lower()\n",
    "    facility = row['Facility'].lower()\n",
    "    dirty_groundtruth.add((player, team, facility))\n",
    "print(\"the dirty groundtruth size is:\", len(dirty_groundtruth))\n",
    "\n",
    "gt_table = pd.read_csv(r\"../data/em_gold/em_stadium_gold_complete.csv\")\n",
    "clean_groundtruth = set()\n",
    "for index, row in gt_table.iterrows():\n",
    "    player = row['Player'].lower()\n",
    "    team = row['Team'].lower()\n",
    "    facility = row['Facility'].lower()\n",
    "    clean_groundtruth.add((player, team, facility))\n",
    "print(\"the clean groundtruth size is:\", len(clean_groundtruth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare integrated table for comparison for both dirty (A) and clean (ltable_cleaned) tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integration_result_dirty = set()\n",
    "for index, row in A.iterrows():\n",
    "    player = row['player']\n",
    "    team = row['team']\n",
    "    facility = row['facility']\n",
    "    integration_result_dirty.add((player, team, facility))\n",
    "print(\"The dirty result size is:\", len(integration_result_dirty))\n",
    "\n",
    "integration_result_clean = set()\n",
    "for index, row in ltable_cleaned.iterrows():\n",
    "    player = row['player']\n",
    "    team = row['team']\n",
    "    facility = row['facility']\n",
    "    integration_result_clean.add((player, team, facility))\n",
    "print(\"The clean result size is:\", len(integration_result_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dirty table size |T|: \", A.shape[0])\n",
    "print(\"Clean table size |T|: \", ltable_cleaned.shape[0])\n",
    "#print(\"Dirty table intersection with dirty ground truth |T int T*|: \", len(clean_groundtruth.intersection(integration_result_dirty)))\n",
    "print(\"Clean table intersection with clean ground truth |T int T*|: \", len(clean_groundtruth.intersection(integration_result_clean)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = len(clean_groundtruth.intersection(integration_result_clean))/ len(integration_result_clean)\n",
    "recall = len(clean_groundtruth.intersection(integration_result_clean)) / len(clean_groundtruth)\n",
    "f1_score = (2 * precision * recall) / (precision + recall)\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print (\"F1-score = \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0822af77c0478f28fc75a22a78f781852ff3b3e7f1600f0f7be807f0cd90bde5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
